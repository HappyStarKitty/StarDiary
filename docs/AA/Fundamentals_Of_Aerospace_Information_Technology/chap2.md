# Chap2 信息论

## 信息论概述

物质、能量、信息是构成客观世界的三大要素

信息是物质和能量在空间和时间上分布的不均匀程度，或者说信息是关于事物运动的状态和规律

信息论是研究信息的基本性质与度量方法，研究信息的获取、传输、存储和处理的一般规律的科学

消息是信息的载体，信号是消息的载体

!!! note "信息传输系统"
    ![无法显示](figures/chap2/sys.png)

    - 信源：产生信息的源

    - 编码器：将消息变成适合于信道传送的信号的设备

    - 信道：信息传输和存储的媒介

    - 译码器：译码是编码的逆变换，分为信道译码和信源译码

    - 信宿：消息的接收者

## 离散信源及其数学模型

!!! note "信源分类"
    信源是产生消息的源，根据消息集X的不同，分为离散信源和连续信源；根据信源的统计特性，离散信源又分为无记忆信源和有记忆信源

### 离散无记忆信源

!!! definition "离散无记忆信源"
    离散无记忆信源（Discrete Memoryless Source, DMS）：输出单个符号的消息，不同时刻发出的符号之间彼此统计独立，且符号集中的符号数目是有限的或是可数的，数学模型为离散性的概率空间

    $$\begin{bmatrix}X \\ P(X) \end{bmatrix}=\begin{bmatrix}x_1 & x_2 & ... x_i... & x_n  \\ P(x_1) & P(x_2) &...P(x_i)... & P(x_N)\end{bmatrix}$$

    其中$P(x_i)$为信源输出符号消息$x_i$的先验概率；$\sum_{i=1}^n P(x_i)=1$

!!! definition "香农信息量"
    一个随机事件发生某一结果后带来的信息量称为自信息量，简称自信息，定义为其发生概率对数的负值

    $$I(x_i)=\log \frac{1}{P(x_i)}=-\log P(x_i)$$

    - 在事件发生前，表示该事件发生的不确定性
    - 在事件发生后，表示事件发生所提供的信息量
    - 非负可加，<b>一串信号的信息量等于各独立事件信息量的和</b>
    - 消息出现的概率越小，所含信息量越大

信息论中1bit是抽象的信息量单位，计算机术语中的bit是一个二元符号；两种定义的关系为，每个二元符号所能提供的最大平均信息量为1bit

!!! definition "信息熵"
    信息熵表征信源的平均不确定度，使用信息量的数学期望表示，也称为信源的平均自信息量

    $$H(X)=E[-\log P(x_i)]=\sum_i P(x_i)I(x_i)=-\sum_i P(x_i)\log P(x_i)$$

!!! theorem "最大离散熵定理"
    对于离散随机变量，当其可能的取值等概率分布时，其熵达到最大值，即信源X中包含N个不同离散消息时，信息熵H(X)有：

    $$\max H(X)=\log N$$

### 多符号离散平稳信源

!!! definition "多符号离散平稳信源"
    信源每次输出的不是单个符号，而是一个符号序列，如果各维联合概率分布均与时间起点无关，这种完全平稳信源称为离散平稳信源

    信源模型

    $$\begin{bmatrix}XY \\ P(XY)\end{bmatrix}=\left\{\begin{aligned}x_1y_1 & ... & x_1y_m & x_2y_1 & ... & x_2y_m & ... & x_ny_1 & ... & x_ny_m \\ p(x_1y_1) & ... & p(x_1y_m) & p(x_2y_1) & ... & p(x_2y_m) & ... & p(x_ny_1) & ... p(x_ny_m)\end{aligned}\right.$$

!!! formula "二维平稳信源的信息熵"
    联合自信息量

    $$I(x_iy_j)=\log_2 \frac{1}{p(x_iy_j)}$$

    联合信息熵H(XY)

    $$H(XY)=\sum_i \sum_j p(x_iy_j)I(x_iy_j)=-\sum_i \sum_j p(x_iy_j)\log p(x_iy_j) $$

    条件自信息量

    $$I(x_i|y_j)=\log_2 \frac{1}{p(x_iy_j)}$$

    条件熵

    $$H(X|Y)=\sum_i \sum_j p(x_iy_j)I(x_i|y_j)=\sum_i \sum_j p(x_iy_j)\log p(x_i|y_j)$$

!!! formula "条件熵和联合熵的关系"
    $$H(XY)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$

    当X，Y相互独立时

    $$H(XY)=H(X)+H(Y)$$

    $$H(X|Y)=H(X)$$

    $$H(Y|X)=H(Y)$$

### N维离散平稳信源的信息熵

!!! note "N维离散平稳信源"
    信源输出序列$X^N=x_1x_2...x_n$的概率为$p(X^N)=p(x_1)p(x_2|x_1)p(x_3|x_1x_2)...p(x^N|x_1x_2...x_{N-1})$，可得其信息熵

    $$H(X^N)=H(X_1X_2...X_N)=H(X_1)+H(X_2|X_1)+...H(X_N|X_1X_2...X_{N-1})$$

    N长的信源符号序列中平均每个信源符号所携带的信息量称为平均符号熵，表示为

    $$H_N(X)=\frac{1}{N}H(X_1X_2...X_N)$$

    根据条件熵的性质，随着N增大，N维条件熵减小，H(X^N)的增加变缓，$H_N(X)$减小，因此有广义Shannon不等式

    $$H_0(X)\ge H_1(X)\ge H_2(X) \ge ... \ge H_{\infty}(X)$$

    其中$H_0(X)$指X为独立等概率均匀分布信源的熵

## 信道

!!! note "信道的数学模型"
    信道是信息的传输通道，信道可视为一个变换器，其将输入消息x变为输出消息y，以转移信道概率$p(y|x)$来描述信道的统计特性

!!! note "信道的类型"
    - 根据输入和输出信号的特点可分为：
        + 离散信道：输入和输出都是取值离散的随机序列，又称为数字信道
        + 连续信道：输入和输出都是取值连续的随机序列，又称为模拟信道
        + 半连续信道：输入和输出一个是离散的，另一个是连续的
    - 根据统计特性可分为：
        + 无记忆信道：输出仅与当前输入有关，而与过去输入无关
        + 有记忆信道：输出与过去输入和（或）过去输出有关
    - 根据信道有无干扰可分为：
        + 有干扰信道：存在干扰或噪声或两者都有，实际通讯信道一般都是有干扰信道
        + 无干扰信道：不存在干扰或噪声或可忽略不计，计算机和外存设备之间的信道可视为无干扰信道

!!! definition "离散无记忆信道"
    离散无记忆信道的输入和输出都是离散无记忆的单个符号，信道的特性可表示为转移概率矩阵，或称信道矩阵

    $$P=\begin{bmatrix}p(y_1|x_1) & p(y_2|x_1) & ... & p(y_J|x_1) \\ p(y_1|x_2) & p(y_2|x_2) & ... & p(y_J|x_2) \\ ... \\ p(y_1|x_I) & p(y_2|x_I) & ... & p(y_J|x_I)\end{bmatrix}$$

!!! definition "条件自信息量"
    后验概率$p(x_i|y_j)$反映接收端收到信息$y_j$而发送端发出的是$x_i$的概率

    接收端收到$y_j$后，发送端发出的是否为$x_i$尚存在的不确定性为条件自信息量

    $$I(x_i|y_j)=\log \frac{1}{p(x_i|y_j)}=-\log p(x_i|y_j)$$

!!! definition "互信息量"
    在接收端收到$y_j$后，已经消除的不确定性为先验的不确定性减去尚存在的不确定性，接收端获得的信息量即互信息量

    $$I(x_i;y_j)=I(x_i)-I(x_i|y_j)=\log \frac{p(x_i|y_j)}{p(x_i)}$$

    在通讯中，互信息量实际上是信道传递的信息量